{
  
    
        "post0": {
            "title": "NCAA Embeddings",
            "content": ". Summary of findings . One issue with using aggregated regular season statistics in an NCAA tournament model is that it doesn&#39;t account for opponent strength of schedule. We can use embeddings to extend pair-wise comparison methods, like the Bradley-Terry model, to more complex metrics than just wins and losses. These methods assume static team skill, but qualitatively appear more robust than aggregating statistics. . Introduction . I expected to compete in my first Kaggle competitions this March - both the NCAA Men&#39;s and Women&#39;s basketball tournaments. Unfortunately, both tournaments were canceled because of the covid-19 pandemic. Regardless, working through my different ideas was a great way to learn some of the basics and nuances of training an ML model effectively. . The typical successful model on Kaggle is an XGBoost ensemble model aimed at classifying wins and losses of tournament games. The biggest differences generally come down to feature selection and engineering. A key advantage of neural networks is the ability of a large network to learn non-linear relationships, which ultimately reduces the necessity for complex feature engineering. However, training and test data are limited in this case. The most detailed data set extend back to 2003, ~1,140 games. Many of these games have spurious results that the best models would not predict. As expected, a neural network architecture without feature engineering performs much more poorly on this data set; a network with feature engineering performs similarly to an XGBoost model. In future posts, I will explore more complex deep learning architectures such as RNNs for tournament prediction, but here I&#39;ll focus on feature engineering. . The easiest way to generate features for tournament prediction is to average a team&#39;s regular season statistics. The Kaggle data set contains various statistics, but users also commonly generate advanced statistics before aggregation. The aggregated features are useful, but do not compensate for opponent strength. If a team had an easy schedule, it may have artificially higher statistics than another. Some methods like the Bradley-Terry model (implemented in Python here), solve for single-value team strengths using previous comparisons of results across all teams. This implementation solves for team strengths based only on wins and losses and couldn&#39;t possibly distinguish between aspects of a team&#39;s strength (e.g. differentiating defensive skill from offensive skill). . But what if we generalize this concept of pairwise-comparisons using embeddings? Embeddings are can be any length and can be trained to represent complex relationships. This notebook will generate team-level embeddings, representative of regular season data (win/losses, point differential, court location) that could be used to train a tournament model. A simple exploratory analysis suggests that the trained embeddings are a richer representation of the original feature data set, that could be implemented in a tournament model. Future testing and expansion of this method to advanced statistics will be needed to confirm that! . What you will see in this notebook: . Brief data prep - we are only using wins/losses, points, home/away, and team IDs as inputs to the model. Later, I will expand this model to advanced statistics, but training the model on this subset of data allows us to test the concept all the way back to 1985! | Model build - This model is being built with the sole purpose of generating useful embeddings. To achieve that we are training the model to be predictive of features that we would ordinarily use as feature inputs to a real tournament model (in this case, regular season wins and losses). | Training and validation - the model is trained using only regular season data from all years and is validated on a secondary set of tournament data (NIT). This is difficult because we have a slight mismatch between our training and validation data. The validation data is generally similar and likely more representative of the real NCAA tournament. That is okay; the end goal of this model is trained embeddings and not win prediction. | Sense check and exploratory analysis - First thing is to check that predictions from the model are sensible, but what we really care about is the embeddings. Do they carry more useful information than simple aggregations of the data they represent? In short, Yes! | . Note: This work was inspired by this Kaggle notebook, which is the first basketball application I&#39;ve seen of this concept. Here is a second example of embeddings used for baseball and the concept could easily be applied to other sports. . #collapse_hide from pathlib import Path import numpy as np import pandas as pd import tensorflow as tf import keras as k from keras.models import Model from keras.layers import Dense, Input, Dropout, Multiply, Lambda, Concatenate, Add, Subtract from keras.layers.embeddings import Embedding from keras.initializers import glorot_uniform, glorot_normal, Constant from keras.optimizers import Adam import matplotlib.pyplot as plt from scipy import stats from sklearn.manifold import TSNE import altair as alt np.random.seed(13) . . Input data . Kaggle provides two regular season data sets: one with less detailed statistics dating back to 1985, and one with more detailed statistics dating back to 2003. For this proof of concept, I&#39;ll simplify the model by working with the older data set. The model will be trained on less powerful statistics, but we will have more historical context during data exploration. Let&#39;s preview the first few rows of the regular season data: . #collapse_hide dataLoc=Path(&#39;./data/2020-05-04-NCAA-Embeddings/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage2/&#39;) df_teams = pd.read_csv(dataLoc/&#39;MTeams.csv&#39;) teams_dict = df_teams[[&#39;TeamID&#39;,&#39;TeamName&#39;]].set_index(&#39;TeamID&#39;).to_dict()[&#39;TeamName&#39;] df_regSeason_data = pd.read_csv(dataLoc/&#39;MRegularSeasonCompactResults.csv&#39;) df_regSeason_data.head() # cols = Season,DayNum,WTeamID,WScore,LTeamID,LScore,WLoc,NumOT . . Season DayNum WTeamID WScore LTeamID LScore WLoc NumOT . 0 1985 | 20 | 1228 | 81 | 1328 | 64 | N | 0 | . 1 1985 | 25 | 1106 | 77 | 1354 | 70 | H | 0 | . 2 1985 | 25 | 1112 | 63 | 1223 | 56 | H | 0 | . 3 1985 | 25 | 1165 | 70 | 1432 | 54 | H | 0 | . 4 1985 | 25 | 1192 | 86 | 1447 | 74 | H | 0 | . Each game is listed only once, with separate columns for winner and loser statistics. The data will be reformatted to leave us with the following inputs and outputs to the model: . Team IDs | Team locations (home or away) | Odds of &#39;Team A&#39; winning (classification/logistic regression) | Expected point differential (team A - team B, regression) | . Each game is presented to the model twice, once as &#39;Team A vs. Team B&#39;, and then vice versa. Team IDs also need to be reformatted to differentiate a single school&#39;s team by year (e.g. Duke 2019 $ neq$ Duke 2020). This may be an oversimplification, but is clearly a better assumption than treating the entire history of a school the same. . Finally, we want to use as much of the training data as possible to train the embeddings. Because the true accuracy of the model is insignificant, secondary tournament data (NIT) is used as training validation. This provides a sense that the model is training correctly, but doesn&#39;t require sacrificing valuable regular season or NCAA tournament data in the process. This data set has the same format as the regular season data and is processed accordingly. . #collapse_hide # load validation data df_otherTourney_data = pd.read_csv(dataLoc/&#39;MSecondaryTourneyCompactResults.csv&#39;).drop(columns=&#39;SecondaryTourney&#39;) # Create team encoding that differentiates teams by year and school def newTeamID(df): # df = df.sample(frac=1).reset_index(drop=True) df[&#39;Wnewid&#39;] = df[&#39;Season&#39;].astype(str) + df[&#39;WTeamID&#39;].astype(str) df[&#39;Lnewid&#39;] = df[&#39;Season&#39;].astype(str) + df[&#39;LTeamID&#39;].astype(str) return df df_regSeason_data = newTeamID(df_regSeason_data) df_otherTourney_data = newTeamID(df_otherTourney_data) def idDicts(df): newid_W = list(df[&#39;Wnewid&#39;].unique()) newid_L = list(df[&#39;Lnewid&#39;].unique()) ids = list(set().union(newid_W,newid_L)) ids.sort() oh_to_id = {} id_to_oh = {} for i in range(len(ids)): id_to_oh[ids[i]] = i oh_to_id[i] = ids[i] return oh_to_id, id_to_oh oh_to_id, id_to_oh = idDicts(df_regSeason_data) # add training data in swapped format so network sees both wins and losses def swapConcat_data(df): df[&#39;Wnewid&#39;] = df[&#39;Wnewid&#39;].apply(lambda x: id_to_oh[x]) df[&#39;Lnewid&#39;] = df[&#39;Lnewid&#39;].apply(lambda x: id_to_oh[x]) loc_dict = {&#39;A&#39;:-1,&#39;N&#39;:0,&#39;H&#39;:1} df[&#39;WLoc&#39;] = df[&#39;WLoc&#39;].apply(lambda x: loc_dict[x]) swap_cols = [&#39;Season&#39;, &#39;DayNum&#39;, &#39;LTeamID&#39;, &#39;LScore&#39;, &#39;WTeamID&#39;, &#39;WScore&#39;, &#39;WLoc&#39;, &#39;NumOT&#39;, &#39;Lnewid&#39;, &#39;Wnewid&#39;] df_swap = df[swap_cols].copy() df_swap[&#39;WLoc&#39;] = df_swap[&#39;WLoc&#39;]*-1 df.columns = [x.replace(&#39;WLoc&#39;,&#39;T1_Court&#39;) .replace(&#39;W&#39;,&#39;T1_&#39;) .replace(&#39;L&#39;,&#39;T2_&#39;) for x in list(df.columns)] df_swap.columns = df.columns df = pd.concat([df,df_swap]) df[&#39;Win&#39;] = (df[&#39;T1_Score&#39;]&gt;df[&#39;T2_Score&#39;]).astype(int) df[&#39;Close_Game&#39;]= abs(df[&#39;T1_Score&#39;]-df[&#39;T2_Score&#39;]) &lt;3 df[&#39;Score_diff&#39;] = df[&#39;T1_Score&#39;] - df[&#39;T2_Score&#39;] df[&#39;Score_diff&#39;] = df[&#39;Score_diff&#39;] - (df[&#39;Score_diff&#39;]/df[&#39;Score_diff&#39;].abs()) df[&#39;T2_Court&#39;] = df[&#39;T1_Court&#39;]*-1 df[[&#39;T1_Court&#39;,&#39;T2_Court&#39;]] = df[[&#39;T1_Court&#39;,&#39;T2_Court&#39;]] + 1 cols = df.columns.to_list() df = df[cols].sort_index() df.reset_index(drop=True,inplace=True) return df df_regSeason_full = swapConcat_data(df_regSeason_data.copy().sort_values(by=&#39;DayNum&#39;)) df_otherTourney_full = swapConcat_data(df_otherTourney_data.copy()) # Convert to numpy arrays in correct format def prep_inputs(df,id_to_oh, col_outputs): Xteams = np.stack([df[&#39;T1_newid&#39;].values,df[&#39;T2_newid&#39;].values]).T Xloc = np.stack([df[&#39;T1_Court&#39;].values,df[&#39;T2_Court&#39;].values]).T if len(col_outputs) &lt;2: Y_outputs = df[col_outputs].values Y_outputs = Y_outputs.reshape(len(Y_outputs),1) else: Y_outputs = np.stack([df[x].values for x in col_outputs]) return [Xteams, Xloc], Y_outputs X_train, Y_train = prep_inputs(df_regSeason_full, id_to_oh, [&#39;Win&#39;,&#39;Score_diff&#39;]) X_test, Y_test = prep_inputs(df_otherTourney_full, id_to_oh, [&#39;Win&#39;,&#39;Score_diff&#39;]) # Normalize point outputs - Win/loss unchanged def normalize_outputs(Y_outputs, stats_cache=None): if stats_cache == None: stats_cache = {} stats_cache[&#39;mean&#39;] = np.mean(Y_outputs,axis=1) stats_cache[&#39;var&#39;] = np.var(Y_outputs,axis=1) else: pass numOut = Y_outputs.shape[0] Y_normout = (Y_outputs-stats_cache[&#39;mean&#39;].reshape((numOut,1)))/stats_cache[&#39;var&#39;].reshape((numOut,1)) return Y_normout, stats_cache Y_norm_train, stats_cache_train = normalize_outputs(Y_train,None) Y_norm_test, _ = normalize_outputs(Y_test,stats_cache_train) Y_norm_train[0,:] = Y_train[0,:] Y_norm_test[0,:] = Y_test[0,:] . . Building the model . This model is built with two input types - home/away flags and team IDs. Each input is repeated for each team and is fed through a location embedding layer and a team embedding layer. The location embedding is 1-dimensional and multiplied by each team&#39;s embedding vector element by element. The team embeddings are separately fed through the same two-layers before being subtracted. This subtracted layer finally connects to two output layers - one &#39;softmax&#39; for win/loss prediction and one dense layer with no activation for point prediction. . A lot of the choices I made here are subjective and would need to be tested against some other options. Among the obvious parameters like number of layers or units per layer are a key parameters that have a significant effect and haven&#39;t been fully explored. For example, should the location embedding activations be multiplied, added, or simply concatenated with the teams embedding layer? What should the dimensions of the teams embedding layer be? Some of these choices, like length of embeddings can be mitigated with regularization (dropout, weight decay), but something like how the two embedding layers combine could have a large effect. Regardless, let&#39;s move forward with the model as is. . . #collapse_hide # build model tf.keras.backend.clear_session() def NCAA_Embeddings_Joint(nteams,teamEmb_size): team_input = Input(shape=[2,],dtype=&#39;int32&#39;, name=&#39;team_input&#39;) X_team = Embedding(input_dim=nteams, output_dim=teamEmb_size, input_length=2, embeddings_initializer=glorot_uniform(), name=&#39;team_encoding&#39;)(team_input) loc_input = Input(shape=[2,],dtype=&#39;int32&#39;, name=&#39;loc_input&#39;) X_loc = Embedding(input_dim=3, output_dim=1, input_length=2, embeddings_initializer=Constant(.5), name=&#39;loc_encoding&#39;)(loc_input) X_loc = Lambda(lambda z: k.backend.repeat_elements(z, rep=teamEmb_size, axis=-1))(X_loc) X = Multiply()([X_team,X_loc]) X = Dropout(rate=.5)(X) T1 = Lambda(lambda z: z[:,0,:])(X) T2 = Lambda(lambda z: z[:,1,:])(X) D1 = Dense(units = 20, use_bias=True, activation=&#39;tanh&#39;) DO1 = Dropout(rate=.5) D2 = Dense(units = 10, use_bias=True, activation=&#39;tanh&#39;) DO2 = Dropout(rate=.5) X1 = D1(T1) X1 = DO1(X1) X1 = D2(X1) X1 = DO2(X1) X2 = D1(T2) X2 = DO1(X2) X2 = D2(X2) X2 = DO2(X2) X_sub = Subtract()([X1,X2]) output_p= Dense(units = 1, use_bias=False, activation=None, name=&#39;point_output&#39;)(X_sub) output_w= Dense(units = 1, use_bias=False, activation=&#39;sigmoid&#39;, name=&#39;win_output&#39;)(X_sub) model = Model(inputs=[team_input, loc_input],outputs=[output_w,output_p],name=&#39;ncaa_embeddings_joint&#39;) return model mymodel = NCAA_Embeddings_Joint(len(id_to_oh),8) mymodel.summary() . . WARNING:tensorflow:From /Users/ryanarmstrong/opt/miniconda3/envs/ds37/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. Model: &#34;ncaa_embeddings_joint&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== loc_input (InputLayer) (None, 2) 0 __________________________________________________________________________________________________ team_input (InputLayer) (None, 2) 0 __________________________________________________________________________________________________ loc_encoding (Embedding) (None, 2, 1) 3 loc_input[0][0] __________________________________________________________________________________________________ team_encoding (Embedding) (None, 2, 8) 92752 team_input[0][0] __________________________________________________________________________________________________ lambda_1 (Lambda) (None, 2, 8) 0 loc_encoding[0][0] __________________________________________________________________________________________________ multiply_1 (Multiply) (None, 2, 8) 0 team_encoding[0][0] lambda_1[0][0] __________________________________________________________________________________________________ dropout_1 (Dropout) (None, 2, 8) 0 multiply_1[0][0] __________________________________________________________________________________________________ lambda_2 (Lambda) (None, 8) 0 dropout_1[0][0] __________________________________________________________________________________________________ lambda_3 (Lambda) (None, 8) 0 dropout_1[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 20) 180 lambda_2[0][0] lambda_3[0][0] __________________________________________________________________________________________________ dropout_2 (Dropout) (None, 20) 0 dense_1[0][0] dense_1[1][0] __________________________________________________________________________________________________ dense_2 (Dense) (None, 10) 210 dropout_2[0][0] dropout_2[1][0] __________________________________________________________________________________________________ dropout_3 (Dropout) (None, 10) 0 dense_2[0][0] dense_2[1][0] __________________________________________________________________________________________________ subtract_1 (Subtract) (None, 10) 0 dropout_3[0][0] dropout_3[1][0] __________________________________________________________________________________________________ win_output (Dense) (None, 1) 10 subtract_1[0][0] __________________________________________________________________________________________________ point_output (Dense) (None, 1) 10 subtract_1[0][0] ================================================================================================== Total params: 93,165 Trainable params: 93,165 Non-trainable params: 0 __________________________________________________________________________________________________ . Training the model . The model is trained using regular season data and validated using secondary tournament data (not the &#39;Big Dance&#39;). The weights of the two losses are adjusted so that they back-propagate a similar amount of error. Because the point differential data has been normalized, the losses are multiple orders of magnitude less than the log loss metric for wins/losses. Given our goal is not really to predict wins and losses here, we could also train on only the point differential. We will see that the two end up being perfectly correlated anyway, but using both gives us a framework for how this could be implemented with advanced statistics in a future example. . #collapse_show # Joint model optimizer = Adam(learning_rate=.01, beta_1=0.9, beta_2=0.999, amsgrad=False) mymodel.compile(loss=[&#39;binary_crossentropy&#39;,&#39;logcosh&#39;], loss_weights=[0.5,400], optimizer=optimizer, metrics = [&#39;accuracy&#39;]) numBatch = round(X_train[0].shape[0]/50) results = mymodel.fit(X_train, [*Y_norm_train], validation_data=(X_test, [*Y_norm_test]), epochs = 30, batch_size = numBatch,shuffle=True, verbose=True) . . WARNING:tensorflow:From /Users/ryanarmstrong/opt/miniconda3/envs/ds37/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.cast instead. Train on 333760 samples, validate on 3248 samples Epoch 1/30 333760/333760 [==============================] - 2s 5us/step - loss: 1.2979 - win_output_loss: 0.6906 - point_output_loss: 0.0024 - win_output_accuracy: 0.5408 - point_output_accuracy: 0.0483 - val_loss: 0.8859 - val_win_output_loss: 0.6908 - val_point_output_loss: 0.0014 - val_win_output_accuracy: 0.5382 - val_point_output_accuracy: 0.0616 Epoch 2/30 333760/333760 [==============================] - 1s 3us/step - loss: 1.0580 - win_output_loss: 0.6422 - point_output_loss: 0.0019 - win_output_accuracy: 0.6641 - point_output_accuracy: 0.0483 - val_loss: 0.8394 - val_win_output_loss: 0.6707 - val_point_output_loss: 0.0013 - val_win_output_accuracy: 0.5844 - val_point_output_accuracy: 0.0616 Epoch 3/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.9983 - win_output_loss: 0.5979 - point_output_loss: 0.0017 - win_output_accuracy: 0.6844 - point_output_accuracy: 0.0483 - val_loss: 0.8217 - val_win_output_loss: 0.6588 - val_point_output_loss: 0.0012 - val_win_output_accuracy: 0.6195 - val_point_output_accuracy: 0.0616 Epoch 4/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.9602 - win_output_loss: 0.5859 - point_output_loss: 0.0017 - win_output_accuracy: 0.6989 - point_output_accuracy: 0.0483 - val_loss: 0.8071 - val_win_output_loss: 0.6510 - val_point_output_loss: 0.0012 - val_win_output_accuracy: 0.6139 - val_point_output_accuracy: 0.0616 Epoch 5/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.9462 - win_output_loss: 0.5780 - point_output_loss: 0.0016 - win_output_accuracy: 0.7039 - point_output_accuracy: 0.0483 - val_loss: 0.7933 - val_win_output_loss: 0.6423 - val_point_output_loss: 0.0012 - val_win_output_accuracy: 0.6268 - val_point_output_accuracy: 0.0616 Epoch 6/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.9328 - win_output_loss: 0.5739 - point_output_loss: 0.0016 - win_output_accuracy: 0.7085 - point_output_accuracy: 0.0483 - val_loss: 0.8160 - val_win_output_loss: 0.6489 - val_point_output_loss: 0.0012 - val_win_output_accuracy: 0.6250 - val_point_output_accuracy: 0.0616 Epoch 7/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.9497 - win_output_loss: 0.5711 - point_output_loss: 0.0016 - win_output_accuracy: 0.7085 - point_output_accuracy: 0.0483 - val_loss: 0.7755 - val_win_output_loss: 0.6329 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6355 - val_point_output_accuracy: 0.0616 Epoch 8/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.9141 - win_output_loss: 0.5642 - point_output_loss: 0.0016 - win_output_accuracy: 0.7124 - point_output_accuracy: 0.0483 - val_loss: 0.7665 - val_win_output_loss: 0.6253 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6583 - val_point_output_accuracy: 0.0616 Epoch 9/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.9181 - win_output_loss: 0.5619 - point_output_loss: 0.0016 - win_output_accuracy: 0.7138 - point_output_accuracy: 0.0483 - val_loss: 0.7503 - val_win_output_loss: 0.6164 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6792 - val_point_output_accuracy: 0.0616 Epoch 10/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.8924 - win_output_loss: 0.5534 - point_output_loss: 0.0015 - win_output_accuracy: 0.7176 - point_output_accuracy: 0.0483 - val_loss: 0.7732 - val_win_output_loss: 0.6088 - val_point_output_loss: 0.0012 - val_win_output_accuracy: 0.6810 - val_point_output_accuracy: 0.0616 Epoch 11/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.8952 - win_output_loss: 0.5520 - point_output_loss: 0.0016 - win_output_accuracy: 0.7209 - point_output_accuracy: 0.0483 - val_loss: 0.7371 - val_win_output_loss: 0.6057 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6804 - val_point_output_accuracy: 0.0616 Epoch 12/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.8900 - win_output_loss: 0.5540 - point_output_loss: 0.0015 - win_output_accuracy: 0.7215 - point_output_accuracy: 0.0483 - val_loss: 0.7528 - val_win_output_loss: 0.6078 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6773 - val_point_output_accuracy: 0.0616 Epoch 13/30 333760/333760 [==============================] - 1s 4us/step - loss: 0.8793 - win_output_loss: 0.5477 - point_output_loss: 0.0015 - win_output_accuracy: 0.7233 - point_output_accuracy: 0.0483 - val_loss: 0.7266 - val_win_output_loss: 0.6026 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6835 - val_point_output_accuracy: 0.0616 Epoch 14/30 333760/333760 [==============================] - 1s 4us/step - loss: 0.8666 - win_output_loss: 0.5444 - point_output_loss: 0.0015 - win_output_accuracy: 0.7248 - point_output_accuracy: 0.0483 - val_loss: 0.7266 - val_win_output_loss: 0.6017 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6872 - val_point_output_accuracy: 0.0616 Epoch 15/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.8641 - win_output_loss: 0.5392 - point_output_loss: 0.0015 - win_output_accuracy: 0.7251 - point_output_accuracy: 0.0483 - val_loss: 0.7290 - val_win_output_loss: 0.6013 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6866 - val_point_output_accuracy: 0.0616 Epoch 16/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.8680 - win_output_loss: 0.5444 - point_output_loss: 0.0015 - win_output_accuracy: 0.7243 - point_output_accuracy: 0.0483 - val_loss: 0.8162 - val_win_output_loss: 0.6027 - val_point_output_loss: 0.0013 - val_win_output_accuracy: 0.6909 - val_point_output_accuracy: 0.0616 Epoch 17/30 333760/333760 [==============================] - 1s 4us/step - loss: 0.8827 - win_output_loss: 0.5459 - point_output_loss: 0.0015 - win_output_accuracy: 0.7251 - point_output_accuracy: 0.0483 - val_loss: 0.7484 - val_win_output_loss: 0.6063 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6817 - val_point_output_accuracy: 0.0616 Epoch 18/30 333760/333760 [==============================] - 1s 4us/step - loss: 0.8706 - win_output_loss: 0.5418 - point_output_loss: 0.0015 - win_output_accuracy: 0.7269 - point_output_accuracy: 0.0483 - val_loss: 0.7289 - val_win_output_loss: 0.6012 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6835 - val_point_output_accuracy: 0.0616 Epoch 19/30 333760/333760 [==============================] - 1s 4us/step - loss: 0.8557 - win_output_loss: 0.5376 - point_output_loss: 0.0015 - win_output_accuracy: 0.7283 - point_output_accuracy: 0.0483 - val_loss: 0.7286 - val_win_output_loss: 0.5987 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6884 - val_point_output_accuracy: 0.0616 Epoch 20/30 333760/333760 [==============================] - 2s 6us/step - loss: 0.8516 - win_output_loss: 0.5347 - point_output_loss: 0.0014 - win_output_accuracy: 0.7272 - point_output_accuracy: 0.0483 - val_loss: 0.7281 - val_win_output_loss: 0.6007 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6884 - val_point_output_accuracy: 0.0616 Epoch 21/30 333760/333760 [==============================] - 2s 7us/step - loss: 0.8491 - win_output_loss: 0.5341 - point_output_loss: 0.0014 - win_output_accuracy: 0.7274 - point_output_accuracy: 0.0483 - val_loss: 0.7514 - val_win_output_loss: 0.6003 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6921 - val_point_output_accuracy: 0.0616 Epoch 22/30 333760/333760 [==============================] - 3s 8us/step - loss: 0.8625 - win_output_loss: 0.5446 - point_output_loss: 0.0015 - win_output_accuracy: 0.7293 - point_output_accuracy: 0.0483 - val_loss: 0.7429 - val_win_output_loss: 0.5990 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6903 - val_point_output_accuracy: 0.0616 Epoch 23/30 333760/333760 [==============================] - 2s 5us/step - loss: 0.8728 - win_output_loss: 0.5396 - point_output_loss: 0.0015 - win_output_accuracy: 0.7266 - point_output_accuracy: 0.0483 - val_loss: 0.7210 - val_win_output_loss: 0.5980 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6983 - val_point_output_accuracy: 0.0616 Epoch 24/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.8614 - win_output_loss: 0.5420 - point_output_loss: 0.0015 - win_output_accuracy: 0.7254 - point_output_accuracy: 0.0483 - val_loss: 0.7340 - val_win_output_loss: 0.6033 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6786 - val_point_output_accuracy: 0.0616 Epoch 25/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.8534 - win_output_loss: 0.5405 - point_output_loss: 0.0014 - win_output_accuracy: 0.7278 - point_output_accuracy: 0.0483 - val_loss: 0.7405 - val_win_output_loss: 0.6014 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6810 - val_point_output_accuracy: 0.0616 Epoch 26/30 333760/333760 [==============================] - 1s 4us/step - loss: 0.8520 - win_output_loss: 0.5373 - point_output_loss: 0.0015 - win_output_accuracy: 0.7289 - point_output_accuracy: 0.0483 - val_loss: 0.7255 - val_win_output_loss: 0.6016 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6810 - val_point_output_accuracy: 0.0616 Epoch 27/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.8471 - win_output_loss: 0.5368 - point_output_loss: 0.0014 - win_output_accuracy: 0.7302 - point_output_accuracy: 0.0483 - val_loss: 0.7738 - val_win_output_loss: 0.6038 - val_point_output_loss: 0.0012 - val_win_output_accuracy: 0.6897 - val_point_output_accuracy: 0.0616 Epoch 28/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.8516 - win_output_loss: 0.5344 - point_output_loss: 0.0015 - win_output_accuracy: 0.7290 - point_output_accuracy: 0.0483 - val_loss: 0.7666 - val_win_output_loss: 0.6041 - val_point_output_loss: 0.0012 - val_win_output_accuracy: 0.6847 - val_point_output_accuracy: 0.0616 Epoch 29/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.8660 - win_output_loss: 0.5403 - point_output_loss: 0.0015 - win_output_accuracy: 0.7301 - point_output_accuracy: 0.0483 - val_loss: 0.7302 - val_win_output_loss: 0.6017 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.6860 - val_point_output_accuracy: 0.0616 Epoch 30/30 333760/333760 [==============================] - 1s 3us/step - loss: 0.8442 - win_output_loss: 0.5338 - point_output_loss: 0.0014 - win_output_accuracy: 0.7304 - point_output_accuracy: 0.0483 - val_loss: 0.7245 - val_win_output_loss: 0.5991 - val_point_output_loss: 0.0011 - val_win_output_accuracy: 0.7001 - val_point_output_accuracy: 0.0616 . #collapse_hide accuracy = results.history[&#39;win_output_accuracy&#39;] val_accuracy = results.history[&#39;val_win_output_accuracy&#39;] loss = results.history[&#39;win_output_loss&#39;] val_loss = results.history[&#39;val_win_output_loss&#39;] # summarize history for accuracy plt.plot(accuracy) plt.plot(val_accuracy) plt.title(&#39;Model accuracy&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.xlabel(&#39;Epoch&#39;) plt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;) plt.show() # summarize history for loss plt.plot(loss) plt.plot(val_loss) plt.title(&#39;Model loss&#39;) plt.ylabel(&#39;Loss&#39;) plt.xlabel(&#39;Epoch&#39;) plt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;) plt.show() . . Model predictions . The cross plots for point differential and win/loss are generally well-behaved. The loss and accuracy of the model differ significantly from results on the Kaggle leaderboard for this competition. For reference, anything below a loss of 0.5 would be considered fantastic, and flipping a coin would give you a loss of about 0.69. We see a similar effect in the point spread prediction with a rather loose correlation of 0.47. . However, this model is optimized to train embeddings, not to predict tournament games. In fact, a model trained to predict winners and losers of regular season games would perform poorly on tournament data anyway. Only the best teams play in the tournament - different features matter and making predictions is even harder). So the model predictions aren&#39;t very useful, but the question still remains: are the embeddings meaningful? A comparison of the embeddings to the aggregated data used in training show that embeddings are a richer representation of the regular season data. . #collapse_hide def transform_y(preds,stats_cache): preds = stats_cache[&#39;var&#39;][1] * preds + stats_cache[&#39;mean&#39;][1] return preds preds = mymodel.predict(X_test) x = transform_y(preds[1],stats_cache_train).reshape(-1) y = transform_y(Y_norm_test[1],stats_cache_train).reshape(-1) print(&#39;Pearson coefficient: &#39;, round(stats.pearsonr(x, y)[0]*100)/100) plt.scatter(x, y, alpha=0.08) plt.xlabel(&#39;Predicted point difference&#39;) plt.ylabel(&#39;Actual point difference&#39;) plt.show() . . Pearson coefficient: 0.47 . #collapse_hide x = transform_y(preds[1],stats_cache_train).reshape(-1) y = preds[0].reshape(-1) print(&#39;Pearson coefficient: &#39;, round(stats.pearsonr(x, y)[0]*100)/100) plt.scatter(x, y, alpha=0.08) plt.xlabel(&#39;Predicted point difference&#39;) plt.ylabel(&#39;Predicted Win Probability&#39;) plt.show() . . Pearson coefficient: 1.0 . #collapse_hide ywin = [] yloss = [] for ii in range(len(Y_test[0])): if Y_test[0,ii]==1: ywin += [y[ii]] else: yloss += [y[ii]] plt.hist(yloss,bins=100, alpha=.5, label=&#39;True Losses&#39;) plt.hist(ywin,bins=100, alpha=.5, label=&#39;True Wins&#39;) plt.xlabel(&#39;Predicted Win Probability&#39;) plt.ylabel(&#39;Count&#39;) plt.legend(loc=&quot;upper center&quot;) plt.show() . . As a brief aside: One benefit of designing the network to predict pairwise-comparisons is its symmetry of predictions when the team order is swapped. Other ML models, such as XGBoost, treat the feature inputs of Team A and Team B differently, which can result in varying predictions when the teams are input in reverse order. This can be an issue even when training sets contain matchups and swapped matchups as documented in this discussion. . Exploratory analysis . The we trained two embeddings in this model - a location embeddings with length 1 for each entry and a teams embedding with length 8. The teams embedding is what we want to look at, but we will need to use t-SNE to map the embeddings into length 2 vectors that we can more easily visualize. First let&#39;s take a peek at the location embedding: . #collapse_hide df_locEmb = pd.DataFrame(mymodel.layers[2].get_weights()[0]) df_locEmb.index = [&#39;Away&#39;,&#39;Neutral&#39;,&#39;Home&#39;] df_locEmb.head() . . Remember that this these elements are multiplied by each team for each game. For a game at a neutral site, each team would get an equivalent boost and it would be a wash. However, The model gives a significant multiplier to the home team. It seems that it wouldn&#39;t make much sense to apply a multiplier to a team embedding that can be positive or negative. After all, do we really expect a poor team to be even worse on their home court? It seems more logical that the team and location embeddings should be added, but a quick test showed very similar final results. Another option is to just concatenate the two embeddings and let the network decide. . The real information we are after resides in the team embeddings. To visualize them we need to condense them down to a form that we can plot more easily. T-SNE is a non-linear mapping algorithm that can represent the vectors in 2D space. It tends to keep vectors that are similar in the embedding space clustered together in 2D space. In our case, t-SNE projects the embeddings into a warped space with the strongest teams tending toward one side and the weakest toward the other. . Let&#39;s take a look at some comparisons between our t-SNE embeddings with and the aggregated game statistics of each team. All of these plots (excluding the final plot for 2020) include only the NCAA tournament teams of a given year. The plots below also highlight a few different historical factors. . Tournament seed number | Number of NCAA Tournament games won that season | Tournament winners | Some of the biggest upsets (from these two articles -&gt; 1, 2) | The tournament field by season | All data for all teams in the 2020 regular season | . #collapse_hide embeddings = mymodel.layers[3].get_weights()[0] pd.DataFrame(embeddings).to_csv(&#39;./data/2020-05-04-NCAA-Embeddings/embeddings.csv&#39;) t = TSNE(n_components=2,random_state=13) embed_tsne = t.fit_transform(embeddings) df_regSeason_full[&#39;T1_TeamName&#39;] = df_regSeason_full[&#39;T1_TeamID&#39;].apply(lambda x: teams_dict[x]) + &#39;-&#39; + df_regSeason_full[&#39;Season&#39;].astype(str) df_agg=df_regSeason_full.groupby(&#39;T1_TeamName&#39;).mean() df_agg.reset_index(inplace=True,drop=False) df_agg[[&#39;T1_TeamName&#39;,&#39;Win&#39;,&#39;Score_diff&#39;]] df_agg.drop(columns=&#39;Season&#39;,inplace=True) df_tourney_data = pd.read_csv(dataLoc/&#39;MNCAATourneyCompactResults.csv&#39;) df_tourney_data[&#39;WTeamName&#39;] = df_tourney_data[&#39;WTeamID&#39;].apply(lambda x: teams_dict[x]) + &#39;-&#39; + df_tourney_data[&#39;Season&#39;].astype(str) df_tourney_data[&#39;Wins&#39;] = 0 df_wins = df_tourney_data[[&#39;WTeamName&#39;,&#39;Wins&#39;]].groupby(&#39;WTeamName&#39;).count() tourneyWinners = [df_tourney_data.loc[df_tourney_data[&#39;Season&#39;]==s,&#39;WTeamName&#39;].values[-1] for s in df_tourney_data[&#39;Season&#39;].unique()] df_seeds = pd.read_csv(dataLoc/&#39;MNCAATourneySeeds.csv&#39;) df_seeds[&#39;TeamName&#39;] = df_seeds[&#39;TeamID&#39;].apply(lambda x: teams_dict[x]) + &#39;-&#39; + df_seeds[&#39;Season&#39;].astype(str) df_seeds[&#39;Seed&#39;] = df_seeds[&#39;Seed&#39;].str.extract(r&#39;( d+)&#39;) df_seeds[&#39;WonTourney&#39;] = df_seeds[&#39;TeamName&#39;].apply(lambda x: True if x in tourneyWinners else False) df_seeds = df_seeds[[&#39;TeamName&#39;,&#39;Seed&#39;,&#39;WonTourney&#39;]] df_upsets = pd.read_csv(&#39;./data/2020-05-04-NCAA-Embeddings/Upsets.csv&#39;) df_upsets[&#39;David&#39;]=df_upsets[&#39;David&#39;]+&#39;-&#39;+df_upsets[&#39;Season&#39;].astype(str) df_upsets[&#39;Goliath&#39;]=df_upsets[&#39;Goliath&#39;]+&#39;-&#39;+df_upsets[&#39;Season&#39;].astype(str) upsets = {} for ii in df_upsets[&#39;David&#39;].unique(): upsets[ii] = &#39;Surprise&#39; for ii in df_upsets[&#39;Goliath&#39;].unique(): upsets[ii] = &#39;Bust&#39; df_seeds = pd.merge(left=df_seeds, right=df_wins, how=&#39;left&#39;, left_on=&#39;TeamName&#39;,right_index=True) df_seeds[&#39;Wins&#39;].fillna(0,inplace=True) def upset(x): try: y = upsets[x] except: y = None return y df_seeds[&#39;Upset&#39;] = df_seeds[&#39;TeamName&#39;].apply(lambda x: upset(x)) df = pd.DataFrame(embed_tsne,columns=[&#39;factor1&#39;,&#39;factor2&#39;]) df[&#39;TeamName&#39;] = [str(teams_dict[int(oh_to_id[x][-4:])]) + &#39;-&#39; + oh_to_id[x][:4] for x in df.index] df[&#39;Season&#39;] = [int(oh_to_id[x][:4])for x in df.index] df = pd.merge(left=df, right=df_seeds, how=&#39;left&#39;, on=&#39;TeamName&#39;) df = pd.merge(left=df, right=df_agg, how=&#39;left&#39;, left_on=&#39;TeamName&#39;,right_on=&#39;T1_TeamName&#39;) df = df[[&#39;TeamName&#39;,&#39;Season&#39;,&#39;factor1&#39;,&#39;factor2&#39;,&#39;Win&#39;,&#39;Score_diff&#39;,&#39;Seed&#39;,&#39;Wins&#39;,&#39;Upset&#39;,&#39;WonTourney&#39;]] df.columns = [&#39;TeamName&#39;,&#39;Season&#39;,&#39;factor1&#39;,&#39;factor2&#39;,&#39;RegWins&#39;,&#39;RegPoint_diff&#39;,&#39;Seed&#39;,&#39;TourneyWins&#39;,&#39;Upset&#39;,&#39;WonTourney&#39;] df2020 = df[df[&#39;Season&#39;]==2020].copy() df.dropna(inplace=True,subset=[&#39;Seed&#39;]) df[&#39;TourneyWinsScaled&#39;] = df[&#39;TourneyWins&#39;]/df[&#39;TourneyWins&#39;].max() df[&#39;SeedScaled&#39;] = df[&#39;Seed&#39;].astype(int)/df[&#39;Seed&#39;].astype(int).max() . . How correlated are the embeddings to tournament seeding? . We see a high correlation between the assigned seed and our embeddings. Our embeddings appear to be a better representation of the seeding than the aggregated statistics, which makes sense since our method uses pair-wise comparisons and effectively accounts for team strength while aggregated statistics do not. . #collapse_hide axis_ranges = [[-80,100], [-80,80], [-10,30], [.2,1.2]] def plot_comparison(df, colorBy, orderBy, axis_ranges): selector = alt.selection_single(empty=&#39;all&#39;, fields=[&#39;TeamName&#39;]) base = alt.Chart(df).mark_point(filled=True,size=50).encode( color=alt.condition(selector, colorBy, alt.value(&#39;lightgray&#39;) ), order=orderBy, tooltip=[&#39;TeamName&#39;,&#39;Seed&#39;] ).properties( width=250, height=250 ).add_selection(selector).interactive() chart1 = [alt.X(&#39;factor1:Q&#39;, scale=alt.Scale(domain=axis_ranges[0]), axis=alt.Axis(title=&#39;t-SNE factor 1&#39;)), alt.Y(&#39;factor2:Q&#39;, scale=alt.Scale(domain=axis_ranges[1]), axis=alt.Axis(title=&#39;t-SNE factor 2&#39;))] chart2 = [alt.X(&#39;RegPoint_diff:Q&#39;, scale=alt.Scale(domain=axis_ranges[2]), axis=alt.Axis(title=&#39;Average regular season point differential&#39;)), alt.Y(&#39;RegWins:Q&#39;, scale=alt.Scale(domain=axis_ranges[3]), axis=alt.Axis(format=&#39;%&#39;, title=&#39;Regular eason win percentage&#39;))] return base, chart1, chart2 colorBy = alt.Color(&#39;Seed:Q&#39;, scale=alt.Scale(scheme=&#39;viridis&#39;,reverse=True)) orderBy = alt.Order(&#39;Seed:Q&#39;, sort=&#39;descending&#39;) base, chart1, chart2 = plot_comparison( df,colorBy, orderBy, axis_ranges) base.encode(*chart1) | base.encode(*chart2) . . Are features associated with tournament wins? . The embeddings appear to be far less correlated to the number of games won by tournament than they were with seeds. This is logical since, tournament seeds are fundamentally generated from the same distribution of data we used in training, while the number of tournament wins requires predictive power. There does appear to be a significant reduction in overlap in the t-SNE representation, especially on the low end of the color scale. . #collapse_hide colorBy = alt.Color(&#39;TourneyWins:Q&#39;, scale=alt.Scale(scheme=&#39;viridis&#39;,reverse=False)) orderBy = alt.Order(&#39;TourneyWins:Q&#39;, sort=&#39;ascending&#39;) base, chart1, chart2 = plot_comparison( df,colorBy, orderBy, axis_ranges) base.encode(*chart1) | base.encode(*chart2) . . Are tournament winners differentiated? . Generally, the tournament winners are clustered together in both of the plots below. They appear to be more closely clustered, but it is difficult to tell given the overall difference in point distribution between our two plots. The distinguishing the impact here will likely require two tournament models using these as inputs. . #collapse_hide colorBy = alt.Color(&#39;WonTourney:N&#39;, scale=alt.Scale(scheme=&#39;tableau10&#39;)) orderBy = alt.Order(&#39;WonTourney:N&#39;, sort=&#39;ascending&#39;) base, chart1, chart2 = plot_comparison( df,colorBy, orderBy, axis_ranges) base.encode(*chart1) | base.encode(*chart2) . . Can we predict &quot;the upsets&quot;? . In short, no. These upsets are compiled from here and here - underdogs shown in red. Generally, the model agrees with the experts. These were upsets and wouldn&#39;t have been predicted by this method. If anything this method would likely have predicted no upset with even greater conviction than a model trained on just aggregated points and wins. The only exception to this is the 1986 &quot;upset&quot; of Cleveland State over the Indiana Hoosiers. Both the embeddings model and the aggregated statistics indicate that Cleveland State may have been the better team. Perhaps it was an issue of name recognition that led this to be called an upset. . #collapse_hide colorBy = alt.Color(&#39;Upset:N&#39;, scale=alt.Scale(scheme=&#39;tableau10&#39;)) orderBy = alt.Order(&#39;Upset:N&#39;, sort=&#39;ascending&#39;) base, chart1, chart2 = plot_comparison( df,colorBy, orderBy, axis_ranges) base.encode(*chart1) | base.encode(*chart2) . . How do these features change from year to year? . The plot below is colored by games won - the tournament winner will appear as yellow. The spread of teams is quite variable year to year. Notably, the tournament that the 1985 Villanova team won as a heavy underdog has less spread in the competition than other years. But interestingly, this model suggests that the 1988 tournament had even less spread and relative to the rest of the field the 1988 Kentucky team may have been an even bigger underdog. . #collapse_hide ## create slider select_year = alt.selection_single( name=&#39;select&#39;, fields=[&#39;Season&#39;], init={&#39;Season&#39;: 1985}, bind=alt.binding_range(min=1985, max=2019, step=1)) colorBy = alt.Color(&#39;TourneyWins:Q&#39;, scale=alt.Scale(scheme=&#39;viridis&#39;,reverse=False)) orderBy = alt.Order(&#39;TourneyWins:Q&#39;, sort=&#39;ascending&#39;) base, chart1, chart2 = plot_comparison( df,colorBy, orderBy, axis_ranges) base = base.add_selection(select_year).transform_filter(select_year) base.encode(*chart1) | base.encode(*chart2) . . What would the 2020 tournament have looked like? . The plot below suggests that the four strongest teams from these metrics are Gonzaga, Kansas, Dayton, and Duke. However, we&#39;ve already established that the embeddings alone are not enough to predict winners - we need a more detailed model to do that. Nonetheless, it is interesting to scan which teams this model has in it&#39;s top 50 or so. This year had some surprising teams and some of the names in the chart might surprise you! I will try to include the 2020 teams in future analyses since we missed out on what could have been a very exciting tournament this year. . #collapse_hide colorBy = alt.Color(&#39;TeamName:N&#39;) axis_ranges = [[-80,90],[-80,70],[-30,30],[0,1]] orderBy = alt.Order(&#39;Upset:N&#39;, sort=&#39;ascending&#39;) base, chart1, chart2 = plot_comparison( df2020, colorBy, orderBy, axis_ranges) base.encode(*chart1) | base.encode(*chart2) . . Conclusions . The embeddings appear to have learned which teams are better and which are worse. It seems that they are a better representation of team skill than simply aggregating the statistics used in model training (wins and point differentials). Using the flexibility of embeddings and Keras, the model could be easily generalized to more advanced statistics. I&#39;ll test this in a future post and would be surprised if this concept doesn&#39;t find its way into my 2021 NCAA Tournament models. .",
            "url": "https://rysarmstr.github.io/Data-Bites-Blog/kaggle/2020/05/04/NCAA-Embeddings.html",
            "relUrl": "/kaggle/2020/05/04/NCAA-Embeddings.html",
            "date": " â€¢ May 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Getting Started",
            "content": "Intro . Hello, World! In my first real post I wanted to share some thoughts about resources for getting started in Python or data science. This has been nearly a 3 year process for me and could have been done much more efficiently; in this post I&#39;ll point out a few ways I would do things differently if I were restarting today. Hopefully, some of these tips can be helpful to others who are learning at their own pace. I will try to keep this post updated with new resources as I find them. Keep in mind, as I&#39;m still actively learning about many tools and my insights will no doubt be incomplete. If you&#39;ve been through this process and have recommendations for things to improve or change please share them in the comments! . Python Install and Package Management . TL;DR: Read Ted Petrou&#39;s &quot;Anaconda is bloated.&quot; . First things first - 99% of what I write here will be about Python. As most python beginners do, I started off by downloading Anaconda Navigator to keep things simple. The first mistake I made - against the advice of many experienced python users - was neglecting to use python environments. The process of installing, uninstalling, and reinstalling packages eventually led to a messy environment. It was time to restart. The simplest thing to me seemed to be (not in reality) a reinstall of Python. Instead of learning new Python packages, I was spinning my wheels. Eventually, I found Ted Petrou&#39;s &quot;Anaconda is bloated&quot;, which gives a fantastic overview of how to get a simple, effective data science set-up running in Python. Pairing this with a program like Visual Studio Code can get you coding quickly with the ability to start over fresh when/if the time comes. Next let&#39;s talk a bit about interfaces. . User Interface (IDE) Selection . TL;DR: If starting over today I would install Jupyter Notebooks, but use Visual Studio Code as my daily interface. VS Code is a great text editor, but also gives you the power of IPython Notebooks and the freedom to experiment with other innovative interfaces like Streamlit. . Of course, you can always write Python code in a simple text editor and run your scripts from terminal, but modern IDEs offer a lot of built-in efficiencies. When I started down the path of learning Python, the idea of not having an official IDE, like MATLAB or RStudio, made me a little squeamish. I jumped straight into Jupyter Notebooks for the simplicity and was ecstatic when I learned about Jupyter Labs, which takes the interactivity of Jupyter Notebooks a step closer to the full IDE experience. The IPython Notebook, which is that basis for the Jupyter&#39;s Python interface is widely used across data science platforms like Kaggle and Coursera, which makes the transition from your machine to other platforms more seamless. The only thing I didn&#39;t like about the Jupyter environment was that I was locked into that interface. . What pushed me to try the oft-recommended VSCode was the introduction of a new interactive interface called Streamlit (more on Streamlit in a bit). In order to try Streamlit I needed a text editor that was separate from Jupyter Notebooks. What I found in VSCode was a text editor that runs IPython Notebooks, has a built in terminal for package/environment management (or git), and of course efficiently edits text files. I&#39;m still learning many of the creative editing shortcuts and I&#39;m sure I am not using the extensions to their full capability, but VS Code has become my primary workspace. . Lastly, I&#39;ll touch on Streamlit, a relatively new interface with a completely different take on interactive code. Streamlit runs the full code of your Python script on every pass unlike IPython Notebooks which execute a single cell at a time. Streamlit allows compute-intensive functions to be cached so that they don&#39;t need to be re-run over and over. The shining feature that made me fall in love with Streamlit is its ability to add interactive features like buttons, sliders, drop-downs, or text input with a single line of code. These features make data exploration much faster in Streamlit and also provide a framework to build simple applets for people looking to deploy ML algorithms quickly. This video from the co-founder and CEO gives a great introduction to the interface. . Courses, Podcasts, and Other Resources . TL;DR: Start with sites like Kaggle that incentivize project building. Fill in knowledge gaps with resources like courses, but don&#39;t fall into the trap of doing courses endlessly with no applied experience. . I have spent the last 3 years or so learning python and then expanding into the world of machine learning and now deep learning. I can&#39;t say that it&#39;s been the most efficient process and I think an important take-away is prioritizing efficiency when choosing your learning path. Not everyone has the same amount of time to dedicate. While a bootcamp might get you where you want to be in 6 months, it can be a large time (and monetary) commitment. Career Karma is an interesting place to start if you are interested in bootcamps but don&#39;t know where to start. For the rest of us, Dan Becker (creator of Kaggle Learn) explains in this episode of &quot;Chai Time Data Science&quot; that the first priority for anyone looking to move into a data science career should be building projects. Keeping that in mind, below you will find some of my favorite resources split out into those that are more conducive to building things quickly and others that are more helpful when you need to take a step back and just absorb some of the technical details. . Quick hitters: . SoloLearn - Learn syntax for Python (don&#39;t spend too much time here!) | Agile Geoscience Kata (for the geologically-inclined) - These small coding challenges are a great way to learn some python basics, especially if you are a geoscientist who works with subsurface data. Likely not as useful for others. | Kaggle Learn - these courses have been specifically designed to get users into real use case examples of how machine learning is used to solve problems. The lessons are short, practical, and don&#39;t dwell on the details, but are enough to get you building. Coursera (see below) is a great alternative for more detailed learning. | FastAI - FastAI is organized in a similar way to Kaggle Learn. Jeremy Howard gets you training deep neural networks within the first video lecture and provides insights along the way. Again, Coursera is an alternative reference if you eventually want to learn to build your own networks from scratch. | TWIML Podcast - The TWIML AI podcast offers a large catalog of interviews with data scientists applying ML/DL. These may be difficult to follow along until you have a bit of experience, but the TWIML AI community also offers study groups and other resources that you may find useful. | Kaggle Competitions - As your skill set grows you can start applying what you are learning and discussing your results with the helpful community on Kaggle. My personal favorite is the yearly March Madness competition, which was unfortunately canceled due to the Covid-19 pandemic. Will try again next year! | . More Detailed/Traditional Learning . Machine Learning Guide (Podcast) - Very high level overviews of data science and types of languages/algorithms. Similar content can be found on the web, but this may be a useful resource if you are a beginner looking for something to listen to on your commute or when you can&#39;t be on a computer. | Coursera - If you need more detail than Kaggle Learn the applied lessons, there is a lot of content on Coursera. A few examples of commonly recommended courses: Python for Everybody - The first few courses of this specialization will give you a crash course in python. It will provide more detail and active learning than something like SoloLearn, which is designed to be short and sweet. | Machine Learning great content as an introduction to the fundamentals of machine learning, but unfortunately taught in Octave or MATLAB. There is significant overlap between this course and the Deep Learning course below - you may actually try that first and see if you need to back track. | Deep Learning - This course will introduce fundamental concepts related to building and training deep neural networks. Expect more theory and free-form coding than the FastAI course. As you get into coding in TensorFlow, this intro to TensorFlow fundamentals is also very useful document. | . | . Hopefully, these resources can be of help or at least get you moving in the right direction as you start your personal data science journey! .",
            "url": "https://rysarmstr.github.io/Data-Bites-Blog/python/2020/04/26/Getting-Started.html",
            "relUrl": "/python/2020/04/26/Getting-Started.html",
            "date": " â€¢ Apr 26, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Introduction",
            "content": "Why the blog? . I&#39;ve spent my career studying geology and physics and have a passion for understanding the cause of natural phenomena. I have lots of experience looking at large data sets - seismic, gravity, magnetics, electromagnetics, and miscellaneous subsurface geologic data. Most of these projects have been done in software like MATLAB, ArcGIS, or Spotfire. . More recently, I&#39;ve started learning Python, Git, and various modeling/visualization frameworks to bring my intuitions about data exploration and data analysis into the world of machine learning and replicable data science. Until now, I haven&#39;t had a clear project or structured place to put my side projects. From now on, this blog will serve as that space! Whether it&#39;s simple visualizations with geologic data or building neural networks to predict March Madness brackets, I will aim to document it here! . Where in the world?! . I am currently a Gulf of Mexico exploration geophysicist for Hess Corporation in Houston, TX. I spend most of my time interpreting subsurface data and following seismic data reprocessing projects, but my past experiences have been much more varied. Below (and above!) are a few of my past roles and projects: . Past Experiences at Hess Corporation Building workflows for cataloging well-based seismic amplitude analysis at a regional scale - Gulf of Mexico, USA | Stratigraphic mapping of regional shallow hazards - Tano Basin, Ghana | Data visualization using Spotfire for a geochemistry database | Basin Scale Play Evaluation for New Ventures - Atlantic Margins (Ireland, Brazil, Sierra Leone, South Africa, Mauritania, Senegal) | Prospect analysis and well planning - Offshore Newfoundland, Canada | . | Master&#39;s Thesis and Research Assistant - University of Wyoming Processing and interpretation of airborne electromagnetic survey - Snowy Range, WY | Geophysical data collection (Seismic refraction, ERT, GPR, Magnetics, NMR) - WY and CA | . | Undergraduate senior thesis and IRIS internship on repeating earthquakes near Christchurch, New Zealand - Research at University of Wisconsin - Madison, thesis at Colorado College. | .",
            "url": "https://rysarmstr.github.io/Data-Bites-Blog/2020/04/25/Introduction.html",
            "relUrl": "/2020/04/25/Introduction.html",
            "date": " â€¢ Apr 25, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Ryan Armstrong . Growing up, I always loved physics because of its ability to explain the natural world. Eventually I fell in love with geology because of its unexplained problems and requirement for creative thinking. Recently, Iâ€™ve stumbled into the world of data science and deep learning where all the joys I found in geology and physics seem to collide into one over-arching field. Iâ€™ve spent my career studying large geologic, geophysical, and environmental data sets. Now Iâ€™ve started honing my data manipulation and modeling skills to catch up with my intuitions. . This blog will serve as my personal outlet as I continue to learn more Python, data modeling, and visualization. Please visit this page or visit my LinkedIn for more details on my background in geoscience and physics. .",
          "url": "https://rysarmstr.github.io/Data-Bites-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://rysarmstr.github.io/Data-Bites-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}